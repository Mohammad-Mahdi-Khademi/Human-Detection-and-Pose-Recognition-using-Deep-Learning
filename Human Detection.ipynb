{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tec2BttdbVez"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\mmahd\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from itertools import chain\n",
        "from tensorflow import keras\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_UZevo7rHdm"
      },
      "source": [
        "**Showing Pictures**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "vrEHsTzprGvf",
        "outputId": "09508f65-3088-4869-d3a7-fa17ef75570b"
      },
      "outputs": [],
      "source": [
        "image_path = r\"D:\\uni\\Human Detection\\Test\\8.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "if image is not None:\n",
        "    cv2.imshow(\"Image\", image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "else:\n",
        "    print(f\"Unable to read the image at {image_path}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxX4c-Mbtk-P"
      },
      "source": [
        "**Adding Pre-trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lH-8H2RDbVhK"
      },
      "outputs": [],
      "source": [
        "model = YOLO(r\"D:\\uni\\Human Detection\\Models\\yolov8m.pt\")\n",
        "pose_model = YOLO(r\"D:\\uni\\Human Detection\\Models\\yolov8n-pose.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uy7QObLtu7V"
      },
      "source": [
        "**Model Result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uepyWgHFbVjt",
        "outputId": "34d7eadf-fbb0-4475-fd23-53392a8ad08e"
      },
      "outputs": [],
      "source": [
        "if image is not None:\n",
        "    results = model.predict(image, verbose=False)\n",
        "    results = results[0]\n",
        "\n",
        "    person_count = 0\n",
        "\n",
        "    for box in results.boxes:\n",
        "        class_id = results.names[box.cls[0].item()]\n",
        "        cords = box.xyxy[0].tolist()\n",
        "        cords = [round(x) for x in cords]\n",
        "        conf = round(box.conf[0].item(), 2)\n",
        "\n",
        "        if class_id == \"person\" and conf >= 0.30:\n",
        "            person_count += 1\n",
        "            x1, y1, x2, y2 = cords\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (69, 145, 31), 3)\n",
        "            \n",
        "            bg_width, bg_height = 50, 25\n",
        "            text = f\"{conf}%\"\n",
        "            cv2.rectangle(image, (x1, y1 - bg_height), (x1 + bg_width, y1), (69, 145, 31), -1)\n",
        "            cv2.putText(image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
        "            \n",
        "    total_bg_width, total_bg_height = 370, 60\n",
        "    text_total = f\"{person_count} Person(s) Detected.\"\n",
        "    cv2.rectangle(image, (0, 40 - total_bg_height), (20 + total_bg_width, 40), (0, 0, 0), -1)\n",
        "    cv2.putText(image, text_total, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (69, 145, 31), 3)\n",
        "    \n",
        "    cv2.imshow(\"Result\", image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "else:\n",
        "    print(f\"Unable to read the image at {image_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\mmahd\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\mmahd\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "loaded_model = load_model(r\"D:\\uni\\Human Detection\\Models\\human_pose_detector.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_CLV3tJ5yGEF"
      },
      "outputs": [],
      "source": [
        "folder_path = r\"D:\\uni\\Human Detection\\Test\"\n",
        "images = []\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        image=cv2.resize(img, (28,28))\n",
        "        images.append(image)\n",
        "\n",
        "x_test = np.array(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 264ms/step\n"
          ]
        }
      ],
      "source": [
        "y_predict = loaded_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w0o5xWtkhnG",
        "outputId": "807168a9-d153-4e61-bb92-adf92823107c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[    0.99494  0.00090808   0.0024102  3.6206e-05   0.0017084]\n",
            " [  0.0040358  0.00021988  1.2114e-05     0.99497  0.00076269]\n",
            " [    0.10421      0.3574     0.43843   0.0021873    0.097768]\n",
            " [    0.47534     0.30651    0.019977     0.19802  0.00014626]\n",
            " [  0.0072704   0.0080036     0.95554  1.3314e-07    0.029184]\n",
            " [   0.017749     0.58178    0.012788      0.3872  0.00047562]\n",
            " [  0.0010982    0.053045     0.93989   0.0055352  0.00043535]\n",
            " [    0.98433   0.0025365  0.00054475   0.0015818    0.011008]\n",
            " [    0.93311  0.00033495     0.06645  1.0048e-08  0.00010531]\n",
            " [  0.0023524  0.00090944   0.0019317  0.00013382     0.99467]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(y_predict)\n",
        "y_predict.argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n_0taPUkjfV",
        "outputId": "9249de75-41f0-453d-db2a-274bc5ce8329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0', '3', '2', '0', '2', '1', '2', '0', '0', '4']\n"
          ]
        }
      ],
      "source": [
        "arr = y_predict.argmax(axis=1)\n",
        "arr_as_strings = [str(num) for num in arr]\n",
        "print(arr_as_strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6L8zd0GklKm",
        "outputId": "1d2c1703-dd4e-4e71-d977-96f589a09e00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['walking', 'leaning', 'standing', 'walking', 'standing', 'sitting', 'standing', 'walking', 'walking', 'drinking']\n"
          ]
        }
      ],
      "source": [
        "arr = y_predict.argmax(axis=1)\n",
        "label_map = {0: 'walking', 1: 'sitting', 2: 'standing', 3: 'leaning', 4: 'drinking'}\n",
        "\n",
        "arr_as_strings = [label_map[num] for num in arr]\n",
        "print(arr_as_strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "jzfCAbvH3qZT",
        "outputId": "05fae234-483d-4702-abbe-8e9280855222"
      },
      "outputs": [],
      "source": [
        "BODY_PARTS = {\"Nose\": 0, \"Neck\": 1, \"Right Shoulder\": 2, \"Right Elbow\": 3, \"Right Wrist\": 4,\n",
        "              \"Left Shoulder\": 5, \"Left Elbow\": 6, \"Left Wrist\": 7, \"Right Hip\": 8, \"Right Knee\": 9,\n",
        "              \"Right Ankle\": 10, \"Left Hip\": 11, \"Left Knee\": 12, \"Left Ankle\": 13, \"Right Eye\": 14,\n",
        "              \"Left Eye\": 15, \"Right Ear\": 16, \"Left Ear\": 17, \"Background\": 18}\n",
        "\n",
        "POSE_PAIRS = [[\"Neck\", \"Right Shoulder\"], [\"Neck\", \"Left Shoulder\"], [\"Right Shoulder\", \"Right Elbow\"],\n",
        "              [\"Right Elbow\", \"Right Wrist\"], [\"Left Shoulder\", \"Left Elbow\"], [\"Left Elbow\", \"Left Wrist\"],\n",
        "              [\"Neck\", \"Right Hip\"], [\"Right Hip\", \"Right Knee\"], [\"Right Knee\", \"Right Ankle\"], [\"Neck\", \"Left Hip\"],\n",
        "              [\"Left Hip\", \"Left Knee\"], [\"Left Knee\", \"Left Ankle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"Right Eye\"],\n",
        "              [\"Right Eye\", \"Right Ear\"], [\"Nose\", \"Left Eye\"], [\"Left Eye\", \"Left Ear\"]]\n",
        "\n",
        "inWidth = 368\n",
        "inHeight = 368\n",
        "\n",
        "net = cv2.dnn.readNetFromTensorflow(r\"D:\\uni\\Human Detection\\Models\\graph_opt.pb\")\n",
        "\n",
        "uploaded_pictures = {\"image1.jpg\": r\"D:\\uni\\Human Detection\\Test\\8.jpg\"}\n",
        "\n",
        "for i, file_name in enumerate(uploaded_pictures.values()):\n",
        "    frame = cv2.imread(file_name)\n",
        "    frameWidth = frame.shape[1]\n",
        "    frameHeight = frame.shape[0]\n",
        "\n",
        "    net.setInput(cv2.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
        "    out = net.forward()\n",
        "    out = out[:, :19, :, :]\n",
        "\n",
        "    assert len(BODY_PARTS) == out.shape[1]\n",
        "\n",
        "    points = []\n",
        "    for i in range(len(BODY_PARTS)):\n",
        "        heatMap = out[0, i, :, :]\n",
        "        _, conf, _, point = cv2.minMaxLoc(heatMap)\n",
        "        x = int((frameWidth * point[0]) / out.shape[3])\n",
        "        y = int((frameHeight * point[1]) / out.shape[2])\n",
        "        points.append((x, y) if conf > 0.2 else None)\n",
        "\n",
        "    for pair in POSE_PAIRS:\n",
        "        partFrom = pair[0]\n",
        "        partTo = pair[1]\n",
        "        assert partFrom in BODY_PARTS\n",
        "        assert partTo in BODY_PARTS\n",
        "\n",
        "        idFrom = BODY_PARTS[partFrom]\n",
        "        idTo = BODY_PARTS[partTo]\n",
        "\n",
        "        if points[idFrom] and points[idTo]:\n",
        "            cv2.line(frame, points[idFrom], points[idTo], (255, 0, 0), 3)\n",
        "            cv2.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv2.FILLED)\n",
        "            cv2.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv2.FILLED)\n",
        "\n",
        "    neck_point = points[BODY_PARTS[\"Neck\"]]\n",
        "    left_hip_point = points[BODY_PARTS[\"Left Hip\"]]\n",
        "    right_hip_point = points[BODY_PARTS[\"Right Hip\"]]\n",
        "\n",
        "    if neck_point and left_hip_point and right_hip_point:\n",
        "        neck_y = neck_point[1]\n",
        "        left_hip_y = left_hip_point[1]\n",
        "        right_hip_y = right_hip_point[1]\n",
        "\n",
        "        if left_hip_y < neck_y < right_hip_y:\n",
        "            status = \"Standing\"\n",
        "        else:\n",
        "            status = \"Walking\"\n",
        "    else:\n",
        "        status = \"leaning or sitting\"\n",
        "\n",
        "    cv2.putText(frame, status, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "    cv2.imshow(\"Pose\", frame)\n",
        "    cv2.waitKey(0)\n",
        "cv2.destroyAllWindows() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 D:\\uni\\Human Detection\\Test\\8.jpg: 640x480 1 person, 255.0ms\n",
            "Speed: 9.1ms preprocess, 255.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns\\pose\\predict5\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "img_path = r\"D:\\uni\\Human Detection\\Test\\8.jpg\"\n",
        "pose_result = pose_model(source=img_path, conf=0.5, show=True, save=True)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
